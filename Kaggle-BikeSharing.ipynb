{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_absolute_error, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "from pycausal import prior as p\n",
    "import itertools\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "def examine_graph_continuous(df, prior = None):\n",
    "    tetrad.run(algoId = 'gfci', testId = 'sem-bic', dfs = df,  scoreId = 'sem-bic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               )\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def normalize(a):\n",
    "    return (a - np.min(a)) / (np.max(a) - np.min(a))\n",
    "\n",
    "def make_categorical(df, complete_df, categoricals):   \n",
    "    retval = None\n",
    "    for key in df.columns:\n",
    "        if retval is not None:\n",
    "            if key in categoricals:\n",
    "                retval = np.concatenate((retval, to_categorical(df[key], len(complete_df[key].unique()))), axis = 1)\n",
    "            else:\n",
    "                retval = np.concatenate((retval, df[key].values[...,np.newaxis]), axis = 1)\n",
    "        else:\n",
    "            if key in categoricals:\n",
    "                retval = to_categorical(df[key], len(complete_df[key].unique()))\n",
    "            else:\n",
    "                retval = df[key]\n",
    "    return retval\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    #x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        #x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_bic(df, prior, penalty = 2):\n",
    "\n",
    "    tetrad.run(algoId = 'fges', dfs = df,  scoreId = 'sem-bic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True, penalty = 200\n",
    "              )\n",
    "\n",
    "    \n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC #/ len(df)\n",
    "\n",
    "num_models = 50  \n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "inputs = [\"Temp\", \"Atemp\", \"Humidity\", \"Windspeed\"]\n",
    "target =  [\"Count\"]\n",
    "\n",
    "\n",
    "categoricals = [] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/Desktop/Kaggle/Bike-Sharing-Dataset/hour.csv', names = [\"Temp\", \"Atemp\", \"Humidity\", \"Windspeed\", \"Count\"], usecols = [10, 11, 12, 13, 16])\n",
    "df = df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17379"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256]] ['temp/bike0', 'temp/bike1', 'temp/bike2', 'temp/bike3', 'temp/bike4', 'temp/bike5', 'temp/bike6', 'temp/bike7', 'temp/bike8', 'temp/bike9', 'temp/bike10', 'temp/bike11', 'temp/bike12', 'temp/bike13', 'temp/bike14', 'temp/bike15', 'temp/bike16', 'temp/bike17', 'temp/bike18', 'temp/bike19', 'temp/bike20', 'temp/bike21', 'temp/bike22', 'temp/bike23', 'temp/bike24', 'temp/bike25', 'temp/bike26', 'temp/bike27', 'temp/bike28', 'temp/bike29', 'temp/bike30', 'temp/bike31', 'temp/bike32', 'temp/bike33', 'temp/bike34', 'temp/bike35', 'temp/bike36', 'temp/bike37', 'temp/bike38', 'temp/bike39', 'temp/bike40', 'temp/bike41', 'temp/bike42', 'temp/bike43', 'temp/bike44', 'temp/bike45', 'temp/bike46', 'temp/bike47', 'temp/bike48', 'temp/bike49']\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"188pt\" viewBox=\"0.00 0.00 192.79 188.00\" width=\"193pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>g</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-184 188.7924,-184 188.7924,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- Atemp -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>Atemp</title>\n",
       "<ellipse cx=\"149.6955\" cy=\"-90\" fill=\"none\" rx=\"35.194\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149.6955\" y=\"-86.3\">Atemp</text>\n",
       "</g>\n",
       "<!-- Windspeed -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>Windspeed</title>\n",
       "<ellipse cx=\"50.6955\" cy=\"-18\" fill=\"none\" rx=\"50.8918\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"50.6955\" y=\"-14.3\">Windspeed</text>\n",
       "</g>\n",
       "<!-- Atemp&#45;&gt;Windspeed -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>Atemp-&gt;Windspeed</title>\n",
       "<path d=\"M120.6859,-68.9021C105.5526,-57.8961 87.3164,-44.6334 73.253,-34.4055\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"119.0366,-72.0304 129.1826,-75.0816 123.1539,-66.3692 119.0366,-72.0304\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Temp -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>Temp</title>\n",
       "<ellipse cx=\"151.6955\" cy=\"-18\" fill=\"none\" rx=\"31.6951\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151.6955\" y=\"-14.3\">Temp</text>\n",
       "</g>\n",
       "<!-- Atemp&#45;&gt;Temp -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>Atemp-&gt;Temp</title>\n",
       "<path d=\"M150.4852,-61.5726C150.7203,-53.1084 150.9716,-44.0593 151.184,-36.4133\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.9793,-61.738 150.2002,-71.8314 153.9766,-61.9324 146.9793,-61.738\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Count -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>Count</title>\n",
       "<ellipse cx=\"50.6955\" cy=\"-162\" fill=\"none\" rx=\"32.4942\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"50.6955\" y=\"-158.3\">Count</text>\n",
       "</g>\n",
       "<!-- Humidity -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>Humidity</title>\n",
       "<ellipse cx=\"50.6955\" cy=\"-90\" fill=\"none\" rx=\"44.6926\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"50.6955\" y=\"-86.3\">Humidity</text>\n",
       "</g>\n",
       "<!-- Count&#45;&gt;Humidity -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>Count-&gt;Humidity</title>\n",
       "<path d=\"M50.6955,-133.5726C50.6955,-128.018 50.6955,-122.2114 50.6955,-116.7148\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"47.1956,-133.8313 50.6955,-143.8314 54.1956,-133.8314 47.1956,-133.8313\" stroke=\"#000000\"/>\n",
       "<ellipse cx=\"50.6955\" cy=\"-112.4133\" fill=\"none\" rx=\"4\" ry=\"4\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Humidity&#45;&gt;Windspeed -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>Humidity-&gt;Windspeed</title>\n",
       "<path d=\"M50.6955,-63.8144C50.6955,-57.5677 50.6955,-50.894 50.6955,-44.6276\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<ellipse cx=\"50.6956\" cy=\"-67.8314\" fill=\"none\" rx=\"4\" ry=\"4\" stroke=\"#000000\"/>\n",
       "<ellipse cx=\"50.6956\" cy=\"-40.4133\" fill=\"none\" rx=\"4\" ry=\"4\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal = [['Temp', 'Atemp', 'Humidity', 'Windspeed'], ['Count']]\n",
    "prior = p.knowledge(requiredirect= [('Temp', 'Atemp'),('Windspeed', 'Atemp')],\n",
    "                   addtemporal = temporal\n",
    "                   )\n",
    "g = examine_graph_continuous(df, prior)\n",
    "dot_str = pc.tetradGraphToDot(g)\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "svg_str = graphs[0].create_svg()\n",
    "SVG(svg_str)\n",
    "\n",
    "known_conx = set({})\n",
    "for i in tetrad.getEdges():\n",
    "    if ' --> ' in i:\n",
    "        known_conx.add((i.split(' --> ')[0], i.split(' --> ')[1]))\n",
    "\n",
    "\n",
    "prior = p.knowledge(requiredirect =  list(map(list, known_conx)),)\n",
    "\n",
    "n_holdout = int(len(df) * 0.2)\n",
    "df['Count'] = normalize(df['Count'].values.astype(int))\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "df['Temp'] = df['Temp'].values.astype(float)\n",
    "df['Atemp'] = df['Atemp'].values.astype(float)\n",
    "df['Windspeed'] = df['Windspeed'].values.astype(float)\n",
    "df['Humidity'] = df['Humidity'].values.astype(float)\n",
    "original_df = df.copy()\n",
    "randomize = False\n",
    "if randomize:\n",
    "    layers = [256, 512, 1024, 2048, 4096]\n",
    "    for i in range(num_models):\n",
    "        network = []\n",
    "        for j in range(3):\n",
    "            network.append(layers[random.randint(0,len(layers) -1)])\n",
    "        models.append(network)\n",
    "        model_names.append('temp/random' + str(i))\n",
    "    print(models, model_names)    \n",
    "else:\n",
    "    model_layers = [512,256]\n",
    "    for i in range(num_models):\n",
    "        models.append(model_layers)\n",
    "        model_names.append('temp/bike' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "SVG(svg_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Instance of edu.cmu.tetrad.data.Knowledge2: /knowledge\n",
       "addtemporal\n",
       "\n",
       "\n",
       "forbiddirect\n",
       "\n",
       "requiredirect\n",
       "Windspeed Atemp\n",
       "Temp Atemp\n",
       "Humidity Count\n",
       "Windspeed Humidity\n",
       "Temp Count"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior = p.knowledge(requiredirect= [('Temp', 'Atemp'),('Windspeed', 'Atemp'),('Windspeed', 'Humidity'), ('Temp', 'Count'),('Humidity', 'Count')])\n",
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 Atemp\n",
      "MSE =  0.3384776722482611\n",
      "BIC =  0.1691076254056624\n",
      "COMB =  0.11661631882236259\n",
      "1 0 Atemp\n",
      "MSE =  0.5981163564721865\n",
      "BIC =  0.4598899456254655\n",
      "COMB =  0.5888848658682534\n",
      "2 0 Atemp\n",
      "MSE =  0.7946093411206178\n",
      "BIC =  0.543856954287011\n",
      "COMB =  0.7735061518823788\n",
      "3 1 Count\n",
      "MSE =  0.1302153884266894\n",
      "BIC =  0.6196011227026705\n",
      "COMB =  0.3270506946105788\n",
      "4 1 Humidity\n",
      "MSE =  0.45802131055279827\n",
      "BIC =  0.23765461995968745\n",
      "COMB =  0.26774260467833244\n",
      "5 1 Count\n",
      "MSE =  0.18089189695564195\n",
      "BIC =  0.2572586407772567\n",
      "COMB =  0.2572586407772567\n",
      "6 1 Atemp\n",
      "MSE =  0.15960819581912616\n",
      "BIC =  0.22140861490715452\n",
      "COMB =  0.17302917362163378\n",
      "7 0 Humidity\n",
      "MSE =  0.17177240518445963\n",
      "BIC =  0.2495211608787745\n",
      "COMB =  0.14689007721930267\n",
      "8 0 Count\n",
      "MSE =  0.6869584931151507\n",
      "BIC =  0.2931767653475593\n",
      "COMB =  0.5571602355513683\n",
      "9 0 Humidity\n",
      "MSE =  0.11912664172942196\n",
      "BIC =  0.19354094815560438\n",
      "COMB =  0.13388400200708556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.36377977016243535,\n",
       " 0.32450163980468466,\n",
       " 0.33420227650385526,\n",
       " 0.2758461015733342,\n",
       " 0.25581019245879383)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestMSE = []\n",
    "bestBIC = []\n",
    "bestCOMBO = []\n",
    "for t in range(10):\n",
    "    # let's split our df into two by race.  Let's see what happens if we \n",
    "    df = original_df.copy()\n",
    "\n",
    "\n",
    "\n",
    "    holdout = int(len(df) * 0.2)\n",
    "        #df_test = df[df['charges'] > 0.54].copy()\n",
    "    continuous = [\"Count\",  \"Humidity\", \"Atemp\", \"Windspeed\"]\n",
    "    \n",
    "    \n",
    "    small = random.randint(0,1)\n",
    "    cont = random.randint(0, len(continuous) - 1)\n",
    "    if small == 0:\n",
    "        df_test = df.nsmallest(holdout, continuous[cont])\n",
    "    else:\n",
    "        df_test = df.nlargest(holdout, continuous[cont])\n",
    "    \n",
    "    print(t, small, continuous[cont])\n",
    "\n",
    "    '''\n",
    "        end_idx = len(df) - holdout\n",
    "    cont = random.randint(0, len(continuous) - 1)\n",
    "    start_idx = random.randint(0, end_idx)\n",
    "    print(t, \"Doing range:\",start_idx, start_idx + holdout, \"and \", continuous[cont])\n",
    "    df_test = df.nlargest(len(df) - start_idx, continuous[cont]).nsmallest(holdout, continuous[cont])\n",
    "    '''\n",
    "\n",
    "\n",
    "    \n",
    "    df.drop(df_test.index, inplace = True)\n",
    "    df_test.reset_index(inplace = True)\n",
    "    df.sample(frac= 1).reset_index(inplace = True) # this will shuffle and reset index\n",
    "\n",
    "    x_test = df_test[inputs]\n",
    "    y_test = df_test[target]\n",
    "\n",
    "    causal_split = 0.2\n",
    "    val_split = 0.2\n",
    "    train_split = 1 - (causal_split + val_split)\n",
    "\n",
    "    x_causal = df[inputs][-int(causal_split * len(df)) :]\n",
    "    y_causal = df[target][-int(causal_split * len(df)) :]\n",
    "\n",
    "    x_val = df[inputs][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "    y_val = df[target][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "\n",
    "    x_train = df[inputs][:int(train_split * len(df))]\n",
    "    y_train = df[target][:int(train_split * len(df))]\n",
    "    len(x_causal), len(y_causal), len(x_val), len(y_val), len(x_train), len(y_train)\n",
    "\n",
    "\n",
    "\n",
    "    x_test_NN = x_test\n",
    "    x_causal_NN = x_causal\n",
    "    x_val_NN = x_val\n",
    "    x_train_NN = x_train\n",
    "\n",
    "    verbosity = 0\n",
    "\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        #print(model_name)\n",
    "\n",
    "        if type(models[idx]) is list:\n",
    "            #clear session\n",
    "            keras.backend.clear_session() \n",
    "            #get model according to specification\n",
    "            model = get_model(models[idx], [0.2] * len(models), np.shape(x_train_NN)[1])\n",
    "            callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                         EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "            model.compile(optimizer = optimizers.SGD(lr = 0.001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "            #print(len(X), len(y))\n",
    "            model.fit(x_train_NN, y_train, epochs = 20, validation_data = (x_val_NN, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "        else:\n",
    "            models[idx].fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    generalization = []\n",
    "    metrics = []\n",
    "    proposed = []\n",
    "    x_causal.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        #print(model_name)\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "\n",
    "        y_pred = model.predict(x_test_NN)\n",
    "        generalization.append(mean_squared_error(y_pred, y_test))\n",
    "\n",
    "        #### CHECK FOR CAUSAL METRIC HERE\n",
    "        y_causal_pred = model.predict(x_causal_NN)\n",
    "        causal_targets = pd.DataFrame(y_causal_pred, columns = target)\n",
    "        causal_targets.reset_index(drop=True, inplace = True)\n",
    "        causal_df = x_causal.join(causal_targets)\n",
    "\n",
    "\n",
    "\n",
    "        metrics.append(mean_squared_error(y_causal_pred, y_causal))\n",
    "        #print(x_causal.head)\n",
    "        bic_pred = get_bic(causal_df, prior)\n",
    "        #print(bic_pred, tetrad.getEdges())\n",
    "\n",
    "\n",
    "        proposed.append(bic_pred)\n",
    "\n",
    "    total = normalize(metrics) + normalize(proposed)\n",
    "    nbest = 5\n",
    "    final = pd.DataFrame(np.stack((metrics, proposed, total, normalize(generalization)), axis = 1), columns = ['metrics', 'proposed', 'combined', 'generalization'])\n",
    "    print(\"MSE = \", np.mean(final.nsmallest(nbest, 'metrics')['generalization']))\n",
    "    print(\"BIC = \", np.mean(final.nsmallest(nbest, 'proposed')['generalization']))\n",
    "    print(\"COMB = \",np.mean(final.nsmallest(nbest, 'combined')['generalization']))\n",
    "    bestMSE.append(final.nsmallest(nbest, 'metrics')['generalization'])\n",
    "    bestBIC.append(final.nsmallest(nbest, 'proposed')['generalization'])\n",
    "    bestCOMBO.append(final.nsmallest(nbest, 'combined')['generalization'])\n",
    "    \n",
    "\n",
    "np.mean(bestMSE), np.mean(bestBIC), np.mean(bestCOMBO), np.std(bestMSE), np.std(bestCOMBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "val1 = []\n",
    "for each in bestMSE:\n",
    "    val1.append(np.mean(each))\n",
    "val2 = []\n",
    "for each in bestCOMBO:\n",
    "    val2.append(np.mean(each))\n",
    "\n",
    "val = []\n",
    "for x, y in zip(val1, val2):\n",
    "    val.append([x, y])\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(2.5,3)\n",
    "df = pd.DataFrame(val, columns = ['MSE', 'Proposed'])\n",
    "ax = sns.boxplot(ax = ax, data=df, palette=\"Set2\")\n",
    "fig.savefig('kaggle-bikeshare.pdf')\n",
    "d = dict()\n",
    "d['bestMSE'] = bestMSE\n",
    "d['bestCOMBO'] = bestCOMBO\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('BikeShare.pkl', 'wb') as handle:\n",
    "    pickle.dump(d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "d['bestMSE'] = bestMSE\n",
    "d['bestCOMBO'] = bestCOMBO\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('BikeShare.pkl', 'wb') as handle:\n",
    "    pickle.dump(d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07808901532949614,\n",
       " 0.0734387598754572,\n",
       " 0.011069054665139995,\n",
       " 0.00980908366533528,\n",
       " -0.04650255454038939,\n",
       " 0.050524876424333294)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_average_improvement(df1, df2):\n",
    "    ret = []\n",
    "    for i, j in zip(df1,df2):\n",
    "        ret.append(np.sum(j) - np.sum(i))\n",
    "    return ret\n",
    "\n",
    "improvement = get_average_improvement(bestMSE, bestCOMBO)\n",
    "\n",
    "np.mean(bestMSE), np.mean(bestCOMBO),np.std(bestMSE), np.std(bestCOMBO), np.mean(improvement), np.std(improvement)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
