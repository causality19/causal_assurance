{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/tkyono/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256], [512, 256]] ['temp/abalone0', 'temp/abalone1', 'temp/abalone2', 'temp/abalone3', 'temp/abalone4', 'temp/abalone5', 'temp/abalone6', 'temp/abalone7', 'temp/abalone8', 'temp/abalone9', 'temp/abalone10', 'temp/abalone11', 'temp/abalone12', 'temp/abalone13', 'temp/abalone14', 'temp/abalone15', 'temp/abalone16', 'temp/abalone17', 'temp/abalone18', 'temp/abalone19', 'temp/abalone20', 'temp/abalone21', 'temp/abalone22', 'temp/abalone23', 'temp/abalone24', 'temp/abalone25', 'temp/abalone26', 'temp/abalone27', 'temp/abalone28', 'temp/abalone29', 'temp/abalone30', 'temp/abalone31', 'temp/abalone32', 'temp/abalone33', 'temp/abalone34', 'temp/abalone35', 'temp/abalone36', 'temp/abalone37', 'temp/abalone38', 'temp/abalone39', 'temp/abalone40', 'temp/abalone41', 'temp/abalone42', 'temp/abalone43', 'temp/abalone44', 'temp/abalone45', 'temp/abalone46', 'temp/abalone47', 'temp/abalone48', 'temp/abalone49', 'temp/abalone50', 'temp/abalone51', 'temp/abalone52', 'temp/abalone53', 'temp/abalone54', 'temp/abalone55', 'temp/abalone56', 'temp/abalone57', 'temp/abalone58', 'temp/abalone59', 'temp/abalone60', 'temp/abalone61', 'temp/abalone62', 'temp/abalone63', 'temp/abalone64', 'temp/abalone65', 'temp/abalone66', 'temp/abalone67', 'temp/abalone68', 'temp/abalone69', 'temp/abalone70', 'temp/abalone71', 'temp/abalone72', 'temp/abalone73', 'temp/abalone74', 'temp/abalone75', 'temp/abalone76', 'temp/abalone77', 'temp/abalone78', 'temp/abalone79', 'temp/abalone80', 'temp/abalone81', 'temp/abalone82', 'temp/abalone83', 'temp/abalone84', 'temp/abalone85', 'temp/abalone86', 'temp/abalone87', 'temp/abalone88', 'temp/abalone89', 'temp/abalone90', 'temp/abalone91', 'temp/abalone92', 'temp/abalone93', 'temp/abalone94', 'temp/abalone95', 'temp/abalone96', 'temp/abalone97', 'temp/abalone98', 'temp/abalone99']\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"188pt\" viewBox=\"0.00 0.00 272.94 188.00\" width=\"273pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>g</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-184 268.9429,-184 268.9429,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- Length -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>Length</title>\n",
       "<ellipse cx=\"35.7468\" cy=\"-162\" fill=\"none\" rx=\"35.9954\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"35.7468\" y=\"-158.3\">Length</text>\n",
       "</g>\n",
       "<!-- Rings -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>Rings</title>\n",
       "<ellipse cx=\"115.7468\" cy=\"-90\" fill=\"none\" rx=\"31.3957\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"115.7468\" y=\"-86.3\">Rings</text>\n",
       "</g>\n",
       "<!-- Length&#45;&gt;Rings -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>Length-&gt;Rings</title>\n",
       "<path d=\"M61.2203,-139.0739C73.5209,-128.0033 87.9423,-115.0241 98.8634,-105.1951\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"58.593,-136.7296 53.5014,-146.0209 63.2758,-141.9327 58.593,-136.7296\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Sex -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>Sex</title>\n",
       "<ellipse cx=\"144.7468\" cy=\"-18\" fill=\"none\" rx=\"27\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"144.7468\" y=\"-14.3\">Sex</text>\n",
       "</g>\n",
       "<!-- Length&#45;&gt;Sex -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>Length-&gt;Sex</title>\n",
       "<path d=\"M44.0123,-134.5357C50.5894,-115.6681 61.1265,-90.7417 75.7468,-72 89.2199,-54.729 109.3678,-39.8019 124.4184,-30.0465\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"40.6826,-133.4568 40.849,-144.0504 47.3252,-135.6652 40.6826,-133.4568\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Rings&#45;&gt;Sex -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>Rings-&gt;Sex</title>\n",
       "<path d=\"M122.9154,-72.2022C127.4105,-61.0419 133.1813,-46.7143 137.6607,-35.593\" fill=\"none\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Height -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>Height</title>\n",
       "<ellipse cx=\"123.7468\" cy=\"-162\" fill=\"none\" rx=\"34.394\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123.7468\" y=\"-158.3\">Height</text>\n",
       "</g>\n",
       "<!-- Height&#45;&gt;Rings -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>Height-&gt;Rings</title>\n",
       "<path d=\"M120.6195,-133.8545C119.6701,-125.3097 118.6517,-116.1442 117.7927,-108.4133\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"117.1451,-134.2791 121.7281,-143.8314 124.1023,-133.506 117.1451,-134.2791\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Height&#45;&gt;Sex -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>Height-&gt;Sex</title>\n",
       "<path d=\"M142.5506,-136.6372C147.8985,-127.995 153.0106,-117.9845 155.7468,-108 162.4867,-83.4059 156.2914,-53.8981 150.8331,-35.5442\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"139.5608,-134.8134 136.9818,-145.0896 145.4062,-138.6646 139.5608,-134.8134\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Diameter -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>Diameter</title>\n",
       "<ellipse cx=\"220.7468\" cy=\"-162\" fill=\"none\" rx=\"44.393\" ry=\"18\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.7468\" y=\"-158.3\">Diameter</text>\n",
       "</g>\n",
       "<!-- Diameter&#45;&gt;Rings -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>Diameter-&gt;Rings</title>\n",
       "<path d=\"M189.3117,-140.4445C172.0354,-128.5979 151.1108,-114.2496 136.0016,-103.889\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"187.7367,-143.6083 197.9634,-146.3771 191.6955,-137.8352 187.7367,-143.6083\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- Diameter&#45;&gt;Sex -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>Diameter-&gt;Sex</title>\n",
       "<path d=\"M206.5663,-135.1316C191.1568,-105.9346 166.9907,-60.1462 153.9048,-35.3519\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"203.6647,-137.1326 211.4277,-144.3428 209.8554,-133.8652 203.6647,-137.1326\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_absolute_error, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "from pycausal import prior as p\n",
    "import itertools\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "def examine_graph_continuous(df, prior = None):\n",
    "    tetrad.run(algoId = 'fges', dfs = df,  scoreId = 'sem-bic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               )\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def examine_graph_mixed(df, prior = None):\n",
    "    tetrad.run(algoId = 'fges', dfs = df, scoreId = 'cond-gauss-bic', \n",
    "           priorKnowledge = prior, dataType = 'mixed', numCategoriesToDiscretize = 5,\n",
    "           structurePrior = 1.0, maxDegree = -1, faithfulnessAssumed = True, verbose = True)\n",
    "\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def examine_graph_discrete(df, prior = None):\n",
    "    tetrad.run(algoId = 'fges', dfs = df, scoreId = 'bdeu', priorKnowledge = prior, dataType = 'discrete',\n",
    "               structurePrior = 1.0, samplePrior = 1.0, maxDegree = 3, faithfulnessAssumed = True, verbose = True)\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def normalize(a):\n",
    "    return (a - np.min(a)) / (np.max(a) - np.min(a))\n",
    "\n",
    "def make_categorical(df, complete_df, categoricals):   \n",
    "    retval = None\n",
    "    for key in df.columns:\n",
    "        if retval is not None:\n",
    "            if key in categoricals:\n",
    "                retval = np.concatenate((retval, to_categorical(df[key], len(complete_df[key].unique()))), axis = 1)\n",
    "            else:\n",
    "                retval = np.concatenate((retval, df[key].values[...,np.newaxis]), axis = 1)\n",
    "        else:\n",
    "            if key in categoricals:\n",
    "                retval = to_categorical(df[key], len(complete_df[key].unique()))\n",
    "            else:\n",
    "                retval = df[key]\n",
    "    return retval\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    #x = keras.layers.Dropout(dropouts[0])(x, training=True)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        #x = keras.layers.Dropout(drop)(x, training=True)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_bic(df, prior, penalty = 2):\n",
    "\n",
    "    tetrad.run(algoId = 'fges', dfs = df, scoreId = 'cond-gauss-bic', \n",
    "           priorKnowledge = prior, dataType = 'mixed', numCategoriesToDiscretize = 4,\n",
    "           structurePrior = 1.0, maxDegree = 3, faithfulnessAssumed = True, verbose = True)\n",
    "    \n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC\n",
    "\n",
    "num_models = 100     \n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '5000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "inputs = [\"Sex\", \"Length\", \"Diameter\"]\n",
    "target =  [\"Height\"]\n",
    "\n",
    "inputs = [\"Sex\", \"Length\", \"Diameter\", \"Height\"]\n",
    "target =  [\"Rings\"]\n",
    "categoricals = ['Sex'] \n",
    "\n",
    "\n",
    "df = pd.read_csv('~/Desktop/Kaggle/abalone.data', names = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Rings\"], usecols = [0, 1, 2, 3, 8])\n",
    "label_encoder_list = []\n",
    "#one_hot = ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course']\n",
    "for i,col in enumerate(['Sex']):\n",
    "    label_encoder_list.append(LabelEncoder())\n",
    "    df[col] = label_encoder_list[i].fit_transform(df[col].values)\n",
    "tempForbid = p.ForbiddenWithin(['Diameter', 'Height', 'Length'])\n",
    "temporal = [['Sex', 'Rings'], tempForbid]\n",
    "prior = p.knowledge(requiredirect= [('Sex', 'Height'),('Sex', 'Diameter'), ('Sex', 'Length')],\n",
    "                   addtemporal = temporal\n",
    "                   )\n",
    "\n",
    "g = examine_graph_mixed(df[inputs + target], prior = prior)\n",
    "dot_str = pc.tetradGraphToDot(g)\n",
    "graphs = pydot.graph_from_dot_data(dot_str)\n",
    "svg_str = graphs[0].create_svg()\n",
    "\n",
    "known_conx = set({})\n",
    "for i in tetrad.getEdges():\n",
    "    if ' --> ' in i:\n",
    "        known_conx.add((i.split(' --> ')[0], i.split(' --> ')[1]))\n",
    "\n",
    "\n",
    "prior = p.knowledge(requiredirect =  list(map(list, known_conx)),)\n",
    "\n",
    "n_holdout = 1000\n",
    "df['Rings'] = normalize(df['Rings'])\n",
    "\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "\n",
    "original_df = df.copy()\n",
    "randomize = False\n",
    "if randomize:\n",
    "    layers = [256, 512, 1024, 2048, 4096]\n",
    "    for i in range(num_models):\n",
    "        network = []\n",
    "        for j in range(3):\n",
    "            network.append(layers[random.randint(0,len(layers) -1)])\n",
    "        models.append(network)\n",
    "        model_names.append('temp/random' + str(i))\n",
    "    print(models, model_names)    \n",
    "else:\n",
    "    model_layers = [512,256]\n",
    "    for i in range(num_models):\n",
    "        models.append(model_layers)\n",
    "        model_names.append('temp/abalone' + str(i))\n",
    "\n",
    "print(models, model_names)\n",
    "SVG(svg_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMSE = []\n",
    "bestBIC = []\n",
    "bestCOMBO = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Length\n",
      "MSE =  0.18345534213124992\n",
      "BIC =  0.14606725892602712\n",
      "COMB =  0.13684903194144\n",
      "1 0 Length\n",
      "MSE =  0.22823076101494671\n",
      "BIC =  0.08337929915853777\n",
      "COMB =  0.11489944406189696\n",
      "2 0 Length\n",
      "MSE =  0.27232966681387155\n",
      "BIC =  0.1318140400193762\n",
      "COMB =  0.11266203677726824\n",
      "3 1 Diameter\n",
      "MSE =  0.20648742323984398\n",
      "BIC =  0.35847376992593183\n",
      "COMB =  0.19132180119381567\n",
      "4 1 Height\n",
      "MSE =  0.3253111424948026\n",
      "BIC =  0.3003739392086374\n",
      "COMB =  0.2820889059232354\n",
      "5 0 Diameter\n",
      "MSE =  0.4024227524091965\n",
      "BIC =  0.16392429696468586\n",
      "COMB =  0.19229188518371032\n",
      "6 0 Diameter\n",
      "MSE =  0.37533774937745074\n",
      "BIC =  0.08427944850281095\n",
      "COMB =  0.17661443895750473\n",
      "7 1 Length\n",
      "MSE =  0.36058964718348846\n",
      "BIC =  0.33444435645359827\n",
      "COMB =  0.20549827949854804\n",
      "8 0 Height\n",
      "MSE =  0.24937891487348463\n",
      "BIC =  0.12852030099284312\n",
      "COMB =  0.13100034585053552\n",
      "9 1 Diameter\n",
      "MSE =  0.31527953620210747\n",
      "BIC =  0.38226489607937875\n",
      "COMB =  0.22773882260276465\n",
      "10 1 Length\n",
      "MSE =  0.3128353103426592\n",
      "BIC =  0.4101328864030389\n",
      "COMB =  0.2815404532514228\n",
      "11 1 Height\n",
      "MSE =  0.458383414356278\n",
      "BIC =  0.3806798489083035\n",
      "COMB =  0.39114914170809073\n",
      "12 1 Height\n",
      "MSE =  0.29102692736440894\n",
      "BIC =  0.24662450707647648\n",
      "COMB =  0.20377144878114023\n",
      "13 0 Rings\n",
      "MSE =  0.23070200797705892\n",
      "BIC =  0.12235844309561031\n",
      "COMB =  0.09747498738957305\n",
      "14 1 Length\n",
      "MSE =  0.3791748308553605\n",
      "BIC =  0.4386107169478084\n",
      "COMB =  0.31728634534333067\n",
      "15 0 Diameter\n",
      "MSE =  0.2596069262012318\n",
      "BIC =  0.16763098922780809\n",
      "COMB =  0.10745713026363471\n",
      "16 1 Rings\n",
      "MSE =  0.17166125568476542\n",
      "BIC =  0.15706540444600553\n",
      "COMB =  0.14503400850285253\n",
      "17 0 Height\n",
      "MSE =  0.23317696584052125\n",
      "BIC =  0.06839860438296927\n",
      "COMB =  0.13094234416923073\n",
      "18 1 Height\n",
      "MSE =  0.4221948961327705\n",
      "BIC =  0.355475617179071\n",
      "COMB =  0.34253205802033804\n",
      "19 1 Height\n",
      "MSE =  0.40381648583605656\n",
      "BIC =  0.3214828523150648\n",
      "COMB =  0.3063790133362092\n",
      "20 0 Rings\n",
      "MSE =  0.20191415090761797\n",
      "BIC =  0.10820984460572058\n",
      "COMB =  0.07255215593807765\n",
      "21 1 Length\n",
      "MSE =  0.3906139258904707\n",
      "BIC =  0.3405786706621234\n",
      "COMB =  0.32478325039242933\n",
      "22 0 Length\n",
      "MSE =  0.3075887962649978\n",
      "BIC =  0.16510445187068665\n",
      "COMB =  0.20805428373357682\n",
      "23 0 Rings\n",
      "MSE =  0.22527097652493358\n",
      "BIC =  0.07299885418797454\n",
      "COMB =  0.15378882473574534\n",
      "24 1 Diameter\n",
      "MSE =  0.5553886640692853\n",
      "BIC =  0.5233477488304804\n",
      "COMB =  0.50497052695522\n",
      "25 0 Rings\n",
      "MSE =  0.2528095677560142\n",
      "BIC =  0.10271895843278231\n",
      "COMB =  0.11144153325810535\n",
      "26 0 Rings\n",
      "MSE =  0.24010443571747891\n",
      "BIC =  0.09510901607506979\n",
      "COMB =  0.126847121885354\n",
      "27 1 Rings\n",
      "MSE =  0.3124954146903471\n",
      "BIC =  0.22217114360190227\n",
      "COMB =  0.23822963765654612\n",
      "28 1 Rings\n",
      "MSE =  0.358522218107309\n",
      "BIC =  0.18758921960654662\n",
      "COMB =  0.21401975641164322\n",
      "29 1 Diameter\n",
      "MSE =  0.4164571469428394\n",
      "BIC =  0.4431902880113553\n",
      "COMB =  0.33624108485096416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.31363730321487676,\n",
       " 0.24521621923038334,\n",
       " 0.22410322274669048,\n",
       " 0.17761657123178337,\n",
       " 0.1775997168424776)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "for t in range(30):\n",
    "    # let's split our df into two by race.  Let's see what happens if we \n",
    "    df = original_df.copy()\n",
    "\n",
    "    #df_test = df[df['Height'] < 0.13].copy()\n",
    "\n",
    "    '''\n",
    "    Good ones\n",
    "    #df_test = df.nsmallest(n_holdout, 'Height').copy()\n",
    "    #df_test = df.nlargest(n_holdout, 'Height').copy()\n",
    "    df_test = df.nsmallest(n_holdout, 'Length').copy()\n",
    "    df_test = df.nlargest(n_holdout, 'Length').copy()\n",
    "    df_test = df[df['Sex'] == 0].copy()\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    bad ones\n",
    "    df_test = df[df['Sex'] == 1][:n_holdout].copy()\n",
    "    df_test = df[df['Sex'] == 2].copy()\n",
    "    df_test = df[df['Sex'] == 1].copy()\n",
    "    '''\n",
    "    #df_test = df.nsmallest(n_holdout, 'Length').copy()\n",
    "\n",
    "    holdout = 800\n",
    "        #df_test = df[df['charges'] > 0.54].copy()\n",
    "    continuous = [\"Length\",  \"Diameter\", \"Height\", \"Rings\"]\n",
    "    \n",
    "    \n",
    "    small = random.randint(0,1)\n",
    "    cont = random.randint(0, len(continuous) - 1)\n",
    "    if small == 0:\n",
    "        df_test = df.nsmallest(holdout, continuous[cont])\n",
    "    else:\n",
    "        df_test = df.nlargest(holdout, continuous[cont])\n",
    "    \n",
    "    print(t, small, continuous[cont])\n",
    "\n",
    "    '''\n",
    "        end_idx = len(df) - holdout\n",
    "    cont = random.randint(0, len(continuous) - 1)\n",
    "    start_idx = random.randint(0, end_idx)\n",
    "    print(t, \"Doing range:\",start_idx, start_idx + holdout, \"and \", continuous[cont])\n",
    "    df_test = df.nlargest(len(df) - start_idx, continuous[cont]).nsmallest(holdout, continuous[cont])\n",
    "    '''\n",
    "\n",
    "\n",
    "    \n",
    "    df.drop(df_test.index, inplace = True)\n",
    "    df_test.reset_index(inplace = True)\n",
    "    df.sample(frac= 1).reset_index(inplace = True) # this will shuffle and reset index\n",
    "\n",
    "    x_test = df_test[inputs]\n",
    "    y_test = df_test[target]\n",
    "\n",
    "    causal_split = 0.2\n",
    "    val_split = 0.2\n",
    "    train_split = 1 - (causal_split + val_split)\n",
    "\n",
    "    x_causal = df[inputs][-int(causal_split * len(df)) :]\n",
    "    y_causal = df[target][-int(causal_split * len(df)) :]\n",
    "\n",
    "    x_val = df[inputs][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "    y_val = df[target][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "\n",
    "    x_train = df[inputs][:int(train_split * len(df))]\n",
    "    y_train = df[target][:int(train_split * len(df))]\n",
    "    len(x_causal), len(y_causal), len(x_val), len(y_val), len(x_train), len(y_train)\n",
    "\n",
    "\n",
    "\n",
    "    x_test_NN = make_categorical(x_test, original_df, categoricals)\n",
    "    x_causal_NN = make_categorical(x_causal, original_df, categoricals)\n",
    "    x_val_NN = make_categorical(x_val, original_df, categoricals)\n",
    "    x_train_NN = make_categorical(x_train, original_df, categoricals)\n",
    "\n",
    "    verbosity = 0\n",
    "\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        #print(model_name)\n",
    "\n",
    "        if type(models[idx]) is list:\n",
    "            #clear session\n",
    "            keras.backend.clear_session() \n",
    "            #get model according to specification\n",
    "            model = get_model(models[idx], [0.2] * len(models), np.shape(x_train_NN)[1])\n",
    "            callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                         EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "            model.compile(optimizer = optimizers.SGD(lr = 0.0001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "            #print(len(X), len(y))\n",
    "            model.fit(x_train_NN, y_train, epochs = 20, validation_data = (x_val_NN, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "        else:\n",
    "            models[idx].fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    generalization = []\n",
    "    metrics = []\n",
    "    proposed = []\n",
    "    x_causal.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        #print(model_name)\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "\n",
    "        y_pred = model.predict(x_test_NN)\n",
    "        generalization.append(mean_squared_error(y_pred, y_test))\n",
    "\n",
    "        #### CHECK FOR CAUSAL METRIC HERE\n",
    "        y_causal_pred = model.predict(x_causal_NN)\n",
    "        causal_targets = pd.DataFrame(y_causal_pred, columns = target)\n",
    "        causal_targets.reset_index(drop=True, inplace = True)\n",
    "        causal_df = x_causal.join(causal_targets)\n",
    "\n",
    "\n",
    "\n",
    "        metrics.append(mean_squared_error(y_causal_pred, y_causal))\n",
    "        #print(x_causal.head)\n",
    "        bic_pred = get_bic(causal_df, prior)\n",
    "        #print(bic_pred, tetrad.getEdges())\n",
    "\n",
    "        found_conx = set({})\n",
    "        for i in tetrad.getEdges():\n",
    "            if ' --> ' in i:\n",
    "                found_conx.add((i.split(' --> ')[0], i.split(' --> ')[1]))\n",
    "        found_conx\n",
    "\n",
    "        if found_conx == known_conx:\n",
    "            proposed.append(bic_pred)\n",
    "        else:\n",
    "            print(\"******Found an error\")\n",
    "            # for now just remove bad model.  Will need to add it to distance metric.\n",
    "            metrics = metrics[:-1]\n",
    "            generalization = generalization[:-1]\n",
    "    total = normalize(metrics) + normalize(proposed)\n",
    "    nbest = 10\n",
    "    final = pd.DataFrame(np.stack((metrics, proposed, total, normalize(generalization)), axis = 1), columns = ['metrics', 'proposed', 'combined', 'generalization'])\n",
    "    print(\"MSE = \", np.mean(final.nsmallest(nbest, 'metrics')['generalization']))\n",
    "    print(\"BIC = \", np.mean(final.nsmallest(nbest, 'proposed')['generalization']))\n",
    "    print(\"COMB = \",np.mean(final.nsmallest(nbest, 'combined')['generalization']))\n",
    "    bestMSE.append(final.nsmallest(nbest, 'metrics')['generalization'])\n",
    "    bestBIC.append(final.nsmallest(nbest, 'proposed')['generalization'])\n",
    "    bestCOMBO.append(final.nsmallest(nbest, 'combined')['generalization'])\n",
    "    \n",
    "\n",
    "np.mean(bestMSE), np.mean(bestBIC), np.mean(bestCOMBO), np.std(bestMSE), np.std(bestCOMBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3153010992959631,\n",
       " 0.23254413734235327,\n",
       " 0.17904696450015567,\n",
       " 0.18045056966883397,\n",
       " -0.8275696195360986,\n",
       " 0.5182068433033671)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_average_improvement(df1, df2):\n",
    "    ret = []\n",
    "    for i, j in zip(df1,df2):\n",
    "        ret.append(np.sum(j) - np.sum(i))\n",
    "    return ret\n",
    "\n",
    "improvement = get_average_improvement(bestMSE, bestCOMBO)\n",
    "\n",
    "np.mean(bestMSE), np.mean(bestCOMBO),np.std(bestMSE), np.std(bestCOMBO), np.mean(improvement), np.std(improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bestMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALQAAADFCAYAAAD9s9hWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADLFJREFUeJzt3X2MXXWdx/H3p0zBShspdNwRLB1WMQZJI/H6kFRbJKIQNtWiqxZZZLMrYbOICUYj0faPNjHx+RGVqt3AAgISKl2CQaNQtEndTqU7SLEKyKwtThzoFNtay1z79Y9zbnMtt71nZs59mN/9vJIJd845957vnX74ze/OOed7FBGYpWJWpwswK5MDbUlxoC0pDrQlxYG2pDjQlhQH2pLiQFtSHGhLSl+ndrxgwYIYHBzs1O5thtm2bdszEdHfbLuOBXpwcJChoaFO7d5mGEkjRbbzlMOS0vOBHh8fZ82aNezdu7fTpVgJej7QGzZsYOfOndx9992dLsVK0NOBHh8fZ9OmTUQEDz30kEfpBPR0oDds2EDtfPDDhw97lE5ATwd68+bNVKtVAKrVKps3b+5wRTZdPR3oJUuW0NeX/eWyr6+PJUuWdLgim66eDvSKFSuQBMCsWbO49NJLO1yRTVehQEu6SNJOSY9L+sQxtnmvpB2SHpV0W7lltsb8+fNZtmwZkli6dCmnnHJKp0uyaWp6pFDSCcANwIXALmCrpI0RsaNum7OB64ElETEu6aWtKrhsK1asYNeuXR6dE1Hk0PcbgMcj4kkASbcD7wR21G3zIeCGiBgHiIg/ll1oq8yfP5/Vq1d3ugwrSZEpxxnA7+u+35Uvq/cq4FWSNkvaIumiRi8k6SpJQ5KGxsbGplax2XEUCbQaLDu6mUcfcDZwPrAS+I6kF0xII2JdRFQiotLf3/TEKbNJKxLoXcDCuu9fDjzdYJt7ImIiIn4H7CQLuFlbFQn0VuBsSWdJOhF4P7DxqG1+ALwVQNICsinIk2UWalZE00BHRBW4BrgfeAy4MyIelbRG0vJ8s/uBZyXtAB4APhYRz7aqaLNjUad621UqlfAJ/laUpG0RUWm2XU8fKbT0ONCWlJ4PtK9YSUvPB9pXrKSlpwPtK1bS09OB9hUr6enpQPuKlfT0dKB9xUp6ejrQvmIlPT0daF+xkp6O9bbrFr5iJS0+l8NmBJ/LYT3Jgbak9PwcOnU333wzIyOFWiv/ndHRUQAGBgYm/dxFixZxxRVXTPp5ZXCgraFDhw51uoQpcaATN9WRcu3atQCsWrWqzHJarpTOSZKulDQmaXv+9e/ll2rWXCmdk3J3RMQ1LajRrLCyOid1XK99+LHGyuqcBPBuScOS7pK0sMH6ruycdOjQoRn7AcheqMgIXaRz0v8A34uIQ5KuBm4CLnjBkyLWAesgO1I4yVqPq9c+/FhjpXROiohnI6I2zH0beF055ZlNTimdkyS9rO7b5WQNaczarumUIyKqkmqdk04A1tc6JwFDEbERuDbvolQF9gBXtrBms2MqdGAlIu4D7jtq2eq6x9eTNTw36yifnGRJcaAtKQ60JcWBtqQ40JYUB9qS4kBbUhxoS4oDbUlxoC0pDrQlxYG2pDjQlhQH2pLiQFtSHGhLigNtSSmlc1Lddu+RFJKa9vE1a4Wmga7rnHQxcA6wUtI5DbabB1wL/KLsIs2KKjJCH+mcFBHPA7XOSUdbC3wW+EuJ9ZlNSimdkySdByyMiHuP90Ld2DnJ0lIk0MftnCRpFvAl4KPNXigi1kVEJSIq/f39xas0K6iMzknzgHOBByU9BbwJ2OgPhtYJ0+6cFBHPRcSCiBiMiEFgC7A8InyLK2u7poGOiCpQ65z0GHBnrXNS3i3JrGuU0jnpqOXnT78ss6npunusTLVx+VTV9lVrq9sObpTeOl0X6JGREX79+G+Zfeq8tuyvyl8BeGLPaFv2N7FnX1v206u6LtAAs0+dx2lvf2Ony2iJZ3/kA6mt5JOTLCkOtCXFgbakONCWFAfakuJAW1IcaEuKA21JcaAtKQ60JcWBtqQ40JYUB9qS4kBbUkrpnCTpakmPSNou6eeNGtGYtUPT86HrOiddSHYF+FZJGyNiR91mt0XEt/LtlwNfBC5qQb09y1fyFFPkBP8jnZMAJNU6Jx0JdET8qW77k6nr2zFZo6OjTOzfl+yJ8BN79jH6/OSfNzIywu9+82vOmDu7/KIamF2tAvD800+0ZX+790+U8jpFAt2oc9ILLieR9J/AdcCJwAWNXkjSVcBVAGeeeeZka+15Z8ydzYcXv7TTZbTE14b/WMrrFAn0cTsnHVkQcQNwg6TLgE8BH2ywzTpgHUClUmk4ig8MDHBgD0lfgjVw6kCny0hWGZ2TjnY78K7pFGU2VdPunAQg6ey6by8BflteiWbFNZ1yRERVUq1z0gnA+lrnJGAoIjYC10h6GzABjNNgumHWDqV0ToqIj5Rcl9mU+EihJcWBtqQ40JYUB9qS4kBbUhxoS0pXdh+d2NO+k5Oq+/4MQN+8F7dlfxN79oEPfbdM1wV60aJFbd3fyL7sNMlF7QrZqQNtf4+9pOsC3e7O9rXzfVetWtXW/VpreA5tSXGgLSkOtCXFgbakONCWFAfakuJAW1IcaEtKWZ2TrpO0Q9KwpJ9I8qEw64imga7rnHQxcA6wskGrr4eBSkQsBu4CPlt2oWZFlNU56YG67bcAl5dZpGUdpQ7unyitIUu32b1/gjmj07/fepEpR6POSWccZ/t/A37YaIWkqyQNSRoaGxsrXqVZQaV1TgKQdDlQAZY1Wl+kc5I1NjAwwPOHDyTdCuzEgemf8Vgk0IU6J+V9OT4JLIuIQ9OuzGwKyuqcdB5wI7A8ItKc5NmM0DTQEVEFap2THgPurHVOyntBA3wOmAt8P296vvEYL2fWUmV1TnpbyXWZTYmPFFpSHGhLigNtSXGgLSkOtCXFgbakONCWFAfaktJ1nZPs2Ha38fTRZw5mN95cMKc9Edm9f4KzSngdB3qGaHc/vIn81sgnnt6e/Z5FOe/RgZ4h3POvmGQCPdWbu0/nJu1l3GzdypVMoKfqpJNO6nQJVqJkAu2R0sB/trPEONCWFAfaklJW56Slkn4pqSrpPeWXaVZMWZ2T/h+4Erit7ALNJqOszklP5esOt6BGs8Ja0TnpmNw5yVqtSKALd05qJiLWRUQlIir9/f1TeQmz4yoS6EKdk8y6QSmdk8y6RSmdkyS9XtIu4J+BGyU92sqizY6lrM5JW8mmImYd5SOFlhQH2pLiQFtSHGhLigNtSUnmihVrrNeutXSgraGZeq2lA524XrvW0nNoS4oDbUlxoC0pDrQlxYG2pDjQlhRFdOYe8pLGgMn/xb81FgDPdLqILtRNP5dFEdH0ur2OBbqbSBqKiEqn6+g2M/Hn4imHJcWBtqQ40Jl1nS6gS824n4vn0JYUj9CWFAfakpJ0oCWFpP+u+75P0pike/Pv/0HSvZL+T9IOSfflywclHZS0ve6ra8/DlPTXvMZfSfq+pBd3uqZjkfSgpJb9KTD186EPAOdKmhMRB4ELgd1169cAP46IrwBIWly37omIeG37Sp2Wg7VaJd0KXA18sbZSksg+LyXfHTbpETr3Q+CS/PFK4Ht1615G1rsPgIgYbmNdrfIz4JX5b5nHJH0D+CWwUNJKSY/kI/lnak+QtF/SF/Km9T+R1J8vf62kLZKGJW2QND9ffm3+G204b6+MpJMlrZe0VdLDkt6ZL58j6fZ82zuAOS199xGR7BewH1gM3AW8CNgOnA/cm69/B7AXeAD4JHB6vnwQOJhvX/t6S6ffz/HeZ/7fPuAe4D/y93AYeFO+7nSyxvT9+XY/Bd6VrwvgA/nj1cDX88fDwLL88Rrgy/njp4GT8sen5P/9NHB5bRnwG+Bk4Dpgfb58MVAFKq36WSQ/Qkc26g6Sjc5HtzO7H/hH4NvAq4GHa6MT+ZSj7utnbSx7suZI2g4MkYX2u/nykYjYkj9+PfBgRIxF1q/wVmBpvu4wcEf++BbgzZJeQhbWTfnym+q2HwZulXQ5WUAB3g58Iq/jQbIB5Mz8ObfAkX+Llv4WTH0OXbMR+DzZ6Hxa/YqI2EN2K43b8g+LS4Ft7S5wmo7MoWuyaTMH6hdN4vWaHZy4hOzntBxYJek1+eu/OyJ2NqijbQc7kh+hc+uBNRHxSP1CSRfU/iIgaR7wCrIRLkW/AJZJWpDfN2clUBt9ZwG1mz1dBvw8Ip4DxiW9JV/+L8AmSbOAhRHxAPBxsunFXLLutB/OP4Ai6bz8eQ8BH8iXnUs27WiZnhihI2IX8JUGq14HfF1Slewf9TsRsVXSIPCK/NdnzfqI+GrLi22RiPiDpOvJPi8IuC8i7slXHwBeI2kb8Bzwvnz5B4Fv5f/TPwn8K3ACcEs+JRHwpYjYK2kt8GVgOA/1U8A/Ad8E/kvSMNlnkf9t5fv0oW9D0v6ImNvpOsrQK1MO6xEeoS0pHqEtKQ60JcWBtqQ40JYUB9qS8je5jNdKeRFppAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 180x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "val1 = []\n",
    "for each in bestMSE:\n",
    "    val1.append(np.mean(each))\n",
    "val2 = []\n",
    "for each in bestCOMBO:\n",
    "    val2.append(np.mean(each))\n",
    "\n",
    "val = []\n",
    "for x, y in zip(val1, val2):\n",
    "    val.append([x, y])\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(2.5,3)\n",
    "df = pd.DataFrame(val, columns = ['MSE', 'Proposed'])\n",
    "ax = sns.boxplot(ax = ax, data=df, palette=\"Set2\")\n",
    "fig.savefig('box-abalone.pdf')\n",
    "d = dict()\n",
    "d['bestMSE'] = bestMSE\n",
    "d['bestCOMBO'] = bestCOMBO\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('abalone.pkl', 'wb') as handle:\n",
    "    pickle.dump(d, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(proposed,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(proposed,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(proposed,generalization, '.')\n",
    "plt.plot(proposed, b + m * np.array(proposed), '-')\n",
    "ax.set_xlabel(\"BIC\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(metrics,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(metrics,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(metrics,generalization, '.')\n",
    "plt.plot(metrics, b + m * np.array(metrics), '-')\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "total = normalize(metrics) + normalize(proposed)\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(total,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(total,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(total,generalization, '.')\n",
    "plt.plot(total, b + m * np.array(total), '-')\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = 5\n",
    "final = pd.DataFrame(np.stack((metrics, proposed, total, generalization), axis = 1), columns = ['metrics', 'proposed', 'combined', 'generalization'])\n",
    "print(\"MSE = \", np.sum(final.nsmallest(nbest, 'metrics')['generalization']))\n",
    "print(\"BIC = \", np.sum(final.nsmallest(nbest, 'proposed')['generalization']))\n",
    "print(\"COMB = \",np.sum(final.nsmallest(nbest, 'combined')['generalization']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(original_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
