{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, \\\n",
    "                        Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import keras.optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_absolute_error, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from pycausal import search as s\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "from collections import defaultdict\n",
    "from numpy.polynomial.polynomial import polyfit  \n",
    "from scipy.stats import pearsonr\n",
    "from pylab import text\n",
    "from pycausal import prior as p\n",
    "import itertools\n",
    "\n",
    "# select your GPU Here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #Comment this line out if you want all GPUS (2 hehe)\n",
    "\n",
    "# python full-display web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def get_model(dense, dropouts, inputs):\n",
    "    # dense is an ordered list of the number of dense neurons like [1024, 2048, 1024]\n",
    "    # dropouts is an ordered list of the dropout masks like [0.2, 0.3, 0.4]\n",
    "    inputs = keras.Input(shape = (inputs,))\n",
    "    x = keras.layers.Dense(dense[0], activation = 'relu')(inputs)\n",
    "    x = keras.layers.Dropout(dropouts[0])(x, training=False)\n",
    "    for den, drop in zip(dense[1:], dropouts[1:]):\n",
    "        x = keras.layers.Dense(den, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop)(x, training=False)\n",
    "    outputs = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def get_bic(df, prior, penalty = 2):\n",
    "\n",
    "    tetrad.run(algoId = 'fges', dfs = df,  scoreId = 'sem-bic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True, penalty = 200\n",
    "              )\n",
    "\n",
    "    \n",
    "    BIC = tetrad.getTetradGraph().getAllAttributes().toString()\n",
    "    BIC = float(BIC.split('=')[-1].split('}')[0])\n",
    "    return BIC #/ len(df)\n",
    "\n",
    "def normalize(a):\n",
    "    return (a - np.min(a)) / (np.max(a) - np.min(a))\n",
    "def make_categorical(df, complete_df, categoricals):   \n",
    "    retval = None\n",
    "    for key in df.columns:\n",
    "        if retval is not None:\n",
    "            if key in categoricals:\n",
    "                retval = np.concatenate((retval, to_categorical(df[key], len(complete_df[key].unique()))), axis = 1)\n",
    "            else:\n",
    "                retval = np.concatenate((retval, df[key].values[...,np.newaxis]), axis = 1)\n",
    "        else:\n",
    "            if key in categoricals:\n",
    "                retval = to_categorical(df[key], len(complete_df[key].unique()))\n",
    "            else:\n",
    "                retval = df[key]\n",
    "    return retval\n",
    "num_models = 100\n",
    "pc = pc()\n",
    "pc.start_vm(java_max_heap_size = '21000M')\n",
    "tetrad = s.tetradrunner()\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "\n",
    "\n",
    "\n",
    "randomize = False\n",
    "if randomize:\n",
    "    layers = [256, 512, 1024, 2048, 4096]\n",
    "    for i in range(num_models):\n",
    "        network = []\n",
    "        for j in range(3):\n",
    "            network.append(layers[random.randint(0,len(layers) -1)])\n",
    "        models.append(network)\n",
    "        model_names.append('temp/18sim' + str(i))\n",
    "    print(models, model_names)    \n",
    "else:\n",
    "    model_layers = [512, 256]\n",
    "    for i in range(num_models):\n",
    "        models.append(model_layers)\n",
    "        model_names.append('temp/18sim' + str(i))\n",
    "\n",
    "print(models, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def random_dag(nodes, edges):\n",
    "    \"\"\"Generate a random Directed Acyclic Graph (DAG) with a given number of nodes and edges.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(nodes):\n",
    "        G.add_node(i)\n",
    "    while edges > 0:\n",
    "        a = random.randint(0,nodes-1)\n",
    "        b=a\n",
    "        while b==a:\n",
    "            b = random.randint(0,nodes-1)\n",
    "        G.add_edge(a,b)\n",
    "        if nx.is_directed_acyclic_graph(G):\n",
    "            edges -= 1\n",
    "        else:\n",
    "            # we closed a loop!\n",
    "            G.remove_edge(a,b)\n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from IPython.display import SVG\n",
    "\n",
    "def examine_graph_continuous(df, prior = None):\n",
    "    tetrad.run(algoId = 'fges', dfs = df,  scoreId = 'sem-bic', dataType = 'continuous',\n",
    "               structurePrior = 1.0, samplePrior = 1, maxDegree = -1, maxPathLength = -1, priorKnowledge = prior,\n",
    "               completeRuleSetUsed = False, faithfulnessAssumed = True, verbose = True,\n",
    "               )\n",
    "    return tetrad.getTetradGraph()\n",
    "\n",
    "def gen_data1(list_vertex = [], list_edges = [], mean = 0, var = 1, SIZE = 20000, perturb = []):\n",
    "    g = []\n",
    "    for v in list_vertex:\n",
    "        if v in perturb:\n",
    "            g.append(np.random.normal(mean,var,SIZE))\n",
    "            print(\"perturbing \", v, \"with meanm var = \", mean, var)\n",
    "        else:\n",
    "            g.append(np.random.normal(0,1,SIZE))\n",
    "        \n",
    "    for edge in list_edges:\n",
    "        g[edge[1]] += g[edge[0]]\n",
    "    g = np.swapaxes(g,0,1)\n",
    "    return pd.DataFrame(g, columns = list(map(str, list_vertex)))\n",
    "\n",
    "def gen_data2(list_vertex = [], list_edges = [], mean = 0, var = 1, SIZE = 20000):\n",
    "    g = []\n",
    "    for v in list_vertex:\n",
    "        g.append(np.random.normal(mean,var,SIZE))\n",
    "\n",
    "        \n",
    "    for edge in list_edges:\n",
    "        g[edge[1]] += g[edge[0]]\n",
    "    g = np.swapaxes(g,0,1)\n",
    "    return pd.DataFrame(g, columns = list(map(str, list_vertex)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bestMSE = []\n",
    "bestBIC = []\n",
    "bestCOMBO = []\n",
    "bestWRONG = []\n",
    "bestPOSSIBLE= []\n",
    "worstMSE = []\n",
    "worstBIC = []\n",
    "worstCOMBO = []\n",
    "worstWRONG = []\n",
    "worstPOSSIBLE= []\n",
    "\n",
    "qbestMSE = []\n",
    "qbestBIC = []\n",
    "qbestCOMBO = []\n",
    "qbestWRONG = []\n",
    "qbestPOSSIBLE= []\n",
    "qworstMSE = []\n",
    "qworstBIC = []\n",
    "qworstCOMBO = []\n",
    "qworstWRONG = []\n",
    "qworstPOSSIBLE= []\n",
    "\n",
    "\n",
    "sbestMSE = []\n",
    "sbestBIC = []\n",
    "sbestCOMBO = []\n",
    "sbestWRONG = []\n",
    "sbestPOSSIBLE= []\n",
    "sworstMSE = []\n",
    "sworstBIC = []\n",
    "sworstCOMBO = []\n",
    "sworstWRONG = []\n",
    "sworstPOSSIBLE= []\n",
    "\n",
    "sqbestMSE = []\n",
    "sqbestBIC = []\n",
    "sqbestCOMBO = []\n",
    "sqbestWRONG = []\n",
    "sqbestPOSSIBLE= []\n",
    "sqworstMSE = []\n",
    "sqworstBIC = []\n",
    "sqworstCOMBO = []\n",
    "sqworstWRONG = []\n",
    "sqworstPOSSIBLE= []\n",
    "\n",
    "averageDegree = []\n",
    "targetDegree = []\n",
    "target_inD = []\n",
    "target_outD = []\n",
    "descendants = []\n",
    "graphDiff = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t= 0\n",
    "while (t < 50):\n",
    "    train_size = 10000\n",
    "    nodes = 18\n",
    "    test_mean = 1\n",
    "    test_var = 2\n",
    "    test_size = 2000\n",
    "    \n",
    "    \n",
    "    w_G = random_dag(nodes, random.randint(0, 36))# since max number of edges is n^2\n",
    "    require = []\n",
    "    for i in w_G.edges:\n",
    "        require.append([str(i[0]), str(i[1])])  \n",
    "    w_prior = p.knowledge(requiredirect = require)\n",
    "\n",
    "    \n",
    "    G = random_dag(nodes, random.randint(17,34)) # since max number of edges is n^2\n",
    "    df = gen_data2(np.arange(nodes), G.edges, SIZE = train_size)\n",
    "    require = []\n",
    "    for i in G.edges:\n",
    "        require.append([str(i[0]), str(i[1])])  \n",
    "    prior = p.knowledge(requiredirect = require)\n",
    "    examine_graph_continuous(df, prior)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Check to make sure that graph matches our prior knowledge. Or else abort this test.\n",
    "    a = set()\n",
    "    for i in tetrad.getEdges():\n",
    "        a.add((i[0], i[-1]))\n",
    "    b = set()\n",
    "    for i in require:\n",
    "        b.add((i[0], i[1]))\n",
    "    print(\"A = \", a)\n",
    "    print(\"B = \", b)\n",
    "    if a != b:\n",
    "        print(\"Doesn't match, but let's just try it\")\n",
    "        #continue\n",
    "\n",
    "        # Need to set our inputs and outputs\n",
    "    inputs = set(np.arange(nodes))\n",
    "    #target = str(a.pop()[random.randint(0,1)])\n",
    "    target = str([x for x in G.nodes() if G.out_degree(x)==0 and G.in_degree(x)>=1][0])\n",
    "    inputs.remove(int(target))\n",
    "    inputs = list(map(str, inputs))\n",
    "    \n",
    "    perturb = int(inputs[random.randint(0,nodes - 2)])\n",
    "    df_test = gen_data2(np.arange(nodes), G.edges, mean = test_mean, var = test_var, SIZE = test_size)\n",
    "    sdf_test = gen_data1(np.arange(nodes), G.edges, mean = test_mean, var = test_var, SIZE = test_size, perturb = [perturb])\n",
    "    target = [target]\n",
    "    \n",
    "    print(\"Inputs = \", inputs)\n",
    "    print(\"Target = \", target)\n",
    "    \n",
    "    x_test = df_test[inputs]\n",
    "    y_test = df_test[target]\n",
    "    \n",
    "    sx_test = sdf_test[inputs]\n",
    "    sy_test = sdf_test[target]\n",
    "\n",
    "    causal_split = 0.2\n",
    "    val_split = 0.2\n",
    "    train_split = 1 - (causal_split + val_split)\n",
    "\n",
    "    x_causal = df[inputs][-int(causal_split * len(df)) :]\n",
    "    y_causal = df[target][-int(causal_split * len(df)) :]\n",
    "\n",
    "    x_val = df[inputs][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "    y_val = df[target][int(train_split * len(df)):-int(causal_split * len(df))]\n",
    "\n",
    "    x_train = df[inputs][:int(train_split * len(df))]\n",
    "    y_train = df[target][:int(train_split * len(df))]\n",
    "\n",
    "    verbosity = 0\n",
    "\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if idx % 10 == 0:\n",
    "            print(idx)\n",
    "        if type(models[idx]) is list:\n",
    "            #clear session\n",
    "            keras.backend.clear_session() \n",
    "            #get model according to specification\n",
    "            model = get_model(models[idx], [0.4] * len(models), np.shape(x_train)[1])\n",
    "            callbacks = [ModelCheckpoint(model_name, verbose= verbosity, monitor='val_loss',save_best_only=True), \n",
    "                         EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose= verbosity, mode='auto')]\n",
    "            model.compile(optimizer = optimizers.SGD(lr = 0.0001, momentum = 0.9, ), loss='mean_squared_error', metrics = ['mse'])\n",
    "            #print(len(X), len(y))\n",
    "            model.fit(x_train, y_train, epochs = 20, validation_data = (x_val, y_val), callbacks = callbacks, batch_size = 32, verbose = verbosity)\n",
    "        else:\n",
    "            models[idx].fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    generalization = []\n",
    "    metrics = []\n",
    "    proposed = []\n",
    "    w_proposed = []\n",
    "    x_causal.reset_index(drop=True, inplace = True)\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        #print(model_name)\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "\n",
    "        y_pred = model.predict(x_test)\n",
    "        generalization.append(mean_squared_error(y_pred, y_test))\n",
    "\n",
    "        #### CHECK FOR CAUSAL METRIC HERE\n",
    "        y_causal_pred = model.predict(x_causal)\n",
    "        causal_targets = pd.DataFrame(y_causal_pred, columns = target)\n",
    "        \n",
    "        #causal_targets.reset_index(drop=True, inplace = True)\n",
    "        causal_df = x_causal.join(causal_targets)\n",
    "\n",
    "\n",
    "\n",
    "        metrics.append(mean_squared_error(y_causal_pred, y_causal))\n",
    "\n",
    "        bic_pred = get_bic(causal_df, prior)\n",
    "        proposed.append(bic_pred)\n",
    "        bic_pred = get_bic(causal_df, w_prior)\n",
    "        w_proposed.append(bic_pred)\n",
    "\n",
    "\n",
    "    nbest = 10\n",
    "    print(metrics)\n",
    "    \n",
    "    good_factor = (1 + len(nx.ancestors(G, int(target[0])))) / len(G.nodes)\n",
    "    wrong_factor = (1 + len(nx.ancestors(w_G, int(target[0])))) / len(w_G.nodes)\n",
    "    print(\"Factors = \", good_factor, wrong_factor)\n",
    "    total = normalize(metrics) + normalize(proposed) * good_factor\n",
    "    wrong = normalize(metrics) + normalize(w_proposed) * wrong_factor\n",
    "    final = pd.DataFrame(np.stack((metrics, proposed,  total, wrong, normalize(generalization)), axis = 1), columns = ['metrics', 'proposed', 'combined', 'wrong', 'generalization'])\n",
    "    print(\"MSE = \", np.sum(final.nsmallest(nbest, 'metrics')['generalization'].values))\n",
    "    print(\"BIC = \", np.sum(final.nsmallest(nbest, 'proposed')['generalization'].values))\n",
    "    print(\"COMB = \",np.sum(final.nsmallest(nbest, 'combined')['generalization'].values))\n",
    "    print(\"WRONG = \", np.sum(final.nsmallest(nbest, 'wrong')['generalization'].values))\n",
    "    print(\"Best possible = \",np.sum(final.nsmallest(nbest, 'generalization')['generalization'].values))\n",
    "    bestMSE.append(final.nsmallest(nbest, 'metrics')['generalization'].values)\n",
    "    bestBIC.append(final.nsmallest(nbest, 'proposed')['generalization'].values)\n",
    "    bestCOMBO.append(final.nsmallest(nbest, 'combined')['generalization'].values)\n",
    "    bestWRONG.append(final.nsmallest(nbest, 'wrong')['generalization'].values)\n",
    "    bestPOSSIBLE.append(final.nsmallest(nbest, 'generalization')['generalization'].values)\n",
    "    \n",
    "    print(\"MSE = \", np.sum(final.nlargest(nbest, 'metrics')['generalization'].values))\n",
    "    print(\"BIC = \", np.sum(final.nlargest(nbest, 'proposed')['generalization'].values))\n",
    "    print(\"COMB = \",np.sum(final.nlargest(nbest, 'combined')['generalization'].values))\n",
    "    print(\"WRONG = \",np.sum(final.nlargest(nbest, 'wrong')['generalization'].values))\n",
    "    print(\"Best possible = \",np.sum(final.nlargest(nbest, 'generalization')['generalization'].values))\n",
    "    worstMSE.append(final.nlargest(nbest, 'metrics')['generalization'].values)\n",
    "    worstBIC.append(final.nlargest(nbest, 'proposed')['generalization'].values)\n",
    "    worstCOMBO.append(final.nlargest(nbest, 'combined')['generalization'].values)\n",
    "    worstWRONG.append(final.nlargest(nbest, 'wrong')['generalization'].values)\n",
    "    worstPOSSIBLE.append(final.nlargest(nbest, 'generalization')['generalization'].values)\n",
    "    \n",
    "    \n",
    "    nbest = 25\n",
    "    print(\"MSE = \", np.sum(final.nsmallest(nbest, 'metrics')['generalization'].values))\n",
    "    print(\"BIC = \", np.sum(final.nsmallest(nbest, 'proposed')['generalization'].values))\n",
    "    print(\"COMB = \",np.sum(final.nsmallest(nbest, 'combined')['generalization'].values))\n",
    "    print(\"WRONG = \", np.sum(final.nsmallest(nbest, 'wrong')['generalization'].values))\n",
    "    print(\"Best possible = \",np.sum(final.nsmallest(nbest, 'generalization')['generalization'].values))\n",
    "    qbestMSE.append(final.nsmallest(nbest, 'metrics')['generalization'].values)\n",
    "    qbestBIC.append(final.nsmallest(nbest, 'proposed')['generalization'].values)\n",
    "    qbestCOMBO.append(final.nsmallest(nbest, 'combined')['generalization'].values)\n",
    "    qbestWRONG.append(final.nsmallest(nbest, 'wrong')['generalization'].values)\n",
    "    qbestPOSSIBLE.append(final.nsmallest(nbest, 'generalization')['generalization'].values)\n",
    "    \n",
    "    print(\"MSE = \", np.sum(final.nlargest(nbest, 'metrics')['generalization'].values))\n",
    "    print(\"BIC = \", np.sum(final.nlargest(nbest, 'proposed')['generalization'].values))\n",
    "    print(\"COMB = \",np.sum(final.nlargest(nbest, 'combined')['generalization'].values))\n",
    "    print(\"WRONG = \",np.sum(final.nlargest(nbest, 'wrong')['generalization'].values))\n",
    "    print(\"Best possible = \",np.sum(final.nlargest(nbest, 'generalization')['generalization'].values))\n",
    "    qworstMSE.append(final.nlargest(nbest, 'metrics')['generalization'].values)\n",
    "    qworstBIC.append(final.nlargest(nbest, 'proposed')['generalization'].values)\n",
    "    qworstCOMBO.append(final.nlargest(nbest, 'combined')['generalization'].values)\n",
    "    qworstWRONG.append(final.nlargest(nbest, 'wrong')['generalization'].values)\n",
    "    qworstPOSSIBLE.append(final.nlargest(nbest, 'generalization')['generalization'].values)\n",
    "    \n",
    "    \n",
    "    generalization = []\n",
    "    metrics = []\n",
    "    proposed = []\n",
    "    w_proposed = []\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        #print(model_name)\n",
    "        if type(models[idx]) is list:\n",
    "            keras.backend.clear_session()\n",
    "            model = load_model(model_name)\n",
    "        else:\n",
    "            model = models[idx]\n",
    "\n",
    "        y_pred = model.predict(sx_test)\n",
    "        generalization.append(mean_squared_error(y_pred, sy_test))\n",
    "\n",
    "        #### CHECK FOR CAUSAL METRIC HERE\n",
    "        y_causal_pred = model.predict(x_causal)\n",
    "        causal_targets = pd.DataFrame(y_causal_pred, columns = target)\n",
    "        \n",
    "        #causal_targets.reset_index(drop=True, inplace = True)\n",
    "        causal_df = x_causal.join(causal_targets)\n",
    "\n",
    "\n",
    "        metrics.append(mean_squared_error(y_causal_pred, y_causal))\n",
    "\n",
    "        bic_pred = get_bic(causal_df, prior)\n",
    "        proposed.append(bic_pred)\n",
    "        bic_pred = get_bic(causal_df, w_prior)\n",
    "        w_proposed.append(bic_pred)\n",
    "\n",
    "    nbest = 10\n",
    "    print(metrics)\n",
    "    total = normalize(metrics) + normalize(proposed) * good_factor\n",
    "    wrong = normalize(metrics) + normalize(w_proposed) * wrong_factor\n",
    "    final = pd.DataFrame(np.stack((metrics, proposed,  total, wrong, normalize(generalization)), axis = 1), columns = ['metrics', 'proposed', 'combined', 'wrong', 'generalization'])\n",
    "    print(\"MSE = \", np.sum(final.nsmallest(nbest, 'metrics')['generalization'].values))\n",
    "    print(\"BIC = \", np.sum(final.nsmallest(nbest, 'proposed')['generalization'].values))\n",
    "    print(\"COMB = \",np.sum(final.nsmallest(nbest, 'combined')['generalization'].values))\n",
    "    print(\"WRONG = \", np.sum(final.nsmallest(nbest, 'wrong')['generalization'].values))\n",
    "    print(\"Best possible = \",np.sum(final.nsmallest(nbest, 'generalization')['generalization'].values))\n",
    "    sbestMSE.append(final.nsmallest(nbest, 'metrics')['generalization'].values)\n",
    "    sbestBIC.append(final.nsmallest(nbest, 'proposed')['generalization'].values)\n",
    "    sbestCOMBO.append(final.nsmallest(nbest, 'combined')['generalization'].values)\n",
    "    sbestWRONG.append(final.nsmallest(nbest, 'wrong')['generalization'].values)\n",
    "    sbestPOSSIBLE.append(final.nsmallest(nbest, 'generalization')['generalization'].values)\n",
    "    \n",
    "    print(\"MSE = \", np.sum(final.nlargest(nbest, 'metrics')['generalization'].values))\n",
    "    print(\"BIC = \", np.sum(final.nlargest(nbest, 'proposed')['generalization'].values))\n",
    "    print(\"COMB = \",np.sum(final.nlargest(nbest, 'combined')['generalization'].values))\n",
    "    print(\"WRONG = \",np.sum(final.nlargest(nbest, 'wrong')['generalization'].values))\n",
    "    print(\"Best possible = \",np.sum(final.nlargest(nbest, 'generalization')['generalization'].values))\n",
    "    sworstMSE.append(final.nlargest(nbest, 'metrics')['generalization'].values)\n",
    "    sworstBIC.append(final.nlargest(nbest, 'proposed')['generalization'].values)\n",
    "    sworstCOMBO.append(final.nlargest(nbest, 'combined')['generalization'].values)\n",
    "    sworstWRONG.append(final.nlargest(nbest, 'wrong')['generalization'].values)\n",
    "    sworstPOSSIBLE.append(final.nlargest(nbest, 'generalization')['generalization'].values)\n",
    "    \n",
    "    \n",
    "    nbest = 25\n",
    "    print(\"MSE = \", np.sum(final.nsmallest(nbest, 'metrics')['generalization'].values))\n",
    "    print(\"BIC = \", np.sum(final.nsmallest(nbest, 'proposed')['generalization'].values))\n",
    "    print(\"COMB = \",np.sum(final.nsmallest(nbest, 'combined')['generalization'].values))\n",
    "    print(\"WRONG = \", np.sum(final.nsmallest(nbest, 'wrong')['generalization'].values))\n",
    "    print(\"Best possible = \",np.sum(final.nsmallest(nbest, 'generalization')['generalization'].values))\n",
    "    sqbestMSE.append(final.nsmallest(nbest, 'metrics')['generalization'].values)\n",
    "    sqbestBIC.append(final.nsmallest(nbest, 'proposed')['generalization'].values)\n",
    "    sqbestCOMBO.append(final.nsmallest(nbest, 'combined')['generalization'].values)\n",
    "    sqbestWRONG.append(final.nsmallest(nbest, 'wrong')['generalization'].values)\n",
    "    sqbestPOSSIBLE.append(final.nsmallest(nbest, 'generalization')['generalization'].values)\n",
    "    \n",
    "    print(\"MSE = \", np.sum(final.nlargest(nbest, 'metrics')['generalization'].values))\n",
    "    print(\"BIC = \", np.sum(final.nlargest(nbest, 'proposed')['generalization'].values))\n",
    "    print(\"COMB = \",np.sum(final.nlargest(nbest, 'combined')['generalization'].values))\n",
    "    print(\"WRONG = \",np.sum(final.nlargest(nbest, 'wrong')['generalization'].values))\n",
    "    print(\"Best possible = \",np.sum(final.nlargest(nbest, 'generalization')['generalization'].values))\n",
    "    sqworstMSE.append(final.nlargest(nbest, 'metrics')['generalization'].values)\n",
    "    sqworstBIC.append(final.nlargest(nbest, 'proposed')['generalization'].values)\n",
    "    sqworstCOMBO.append(final.nlargest(nbest, 'combined')['generalization'].values)\n",
    "    sqworstWRONG.append(final.nlargest(nbest, 'wrong')['generalization'].values)\n",
    "    sqworstPOSSIBLE.append(final.nlargest(nbest, 'generalization')['generalization'].values)\n",
    "    \n",
    "    \n",
    "    print(\"Times = \", t)\n",
    "    d = []\n",
    "    for i in G.degree():\n",
    "        d.append(i[1])\n",
    "        if str(i[0]) in target:\n",
    "            targetDegree.append(i[1])\n",
    "    averageDegree.append(np.mean(d))\n",
    "    target_inD.append(G.in_degree(int(target[0])))\n",
    "    target_outD.append(G.out_degree(int(target[0])))\n",
    "    descendants.append(len(nx.descendants(G, perturb)))\n",
    "    \n",
    "    graphDiff.append(len(nx.difference(G, w_G).edges()) + len(nx.difference(w_G, G).edges()) )\n",
    "    print(targetDegree, averageDegree)\n",
    "    t += 1\n",
    "    \n",
    "np.mean(bestMSE), np.mean(bestCOMBO), np.std(bestMSE), np.std(bestCOMBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(bestMSE), np.mean(bestCOMBO), np.mean(sbestMSE), np.mean(sbestCOMBO),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nx.ancestors(G, int(target[0]))) / len(G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bic(causal_df, prior), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.nsmallest(nbest, 'proposed')['generalization'].values, final.nsmallest(nbest, 'wrong')['generalization'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_average_improvement(df1, df2):\n",
    "    ret = []\n",
    "    for i, j in zip(df1,df2):\n",
    "        ret.append(np.sum(j) - np.sum(i))\n",
    "    return ret\n",
    "\n",
    "improvement = get_average_improvement(bestMSE, bestCOMBO)\n",
    "np.mean(improvement), np.std(improvement)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(averageDegree,improvement, '.')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(targetDegree,improvement, '.')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(target_inD,improvement, '.')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(target_outD,improvement, '.')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(descendants,improvement, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for x,y in zip(qbestMSE, descendants):\n",
    "    lst.append(['MSE', np.mean(x), y])\n",
    "for x,y in zip(qbestCOMBO, descendants):\n",
    "    lst.append(['Proposed', np.mean(x), y])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(lst, columns = ['Method', 'L2', 'NodesOUTD'])\n",
    "ax = sns.boxplot(data=df, x = 'NodesOUTD', y = 'L2', hue = 'Method', linewidth = 1.5,palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "for x,y in zip(qworstMSE, descendants):\n",
    "    lst.append(['worst MSE', np.mean(x), y])\n",
    "for x,y in zip(qworstCOMBO, descendants):\n",
    "    lst.append(['worst Proposed', np.mean(x), y])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(lst, columns = ['Method', 'L2', 'NodesOUTD'])\n",
    "ax = sns.boxplot(data=df, x = 'NodesOUTD', y = 'L2', hue = 'Method', linewidth = 1.5,palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "val1 = []\n",
    "for each in sbestMSE:\n",
    "    val1.append(np.mean(each))\n",
    "val2 = []\n",
    "for each in sbestCOMBO:\n",
    "    val2.append(np.mean(each))\n",
    "\n",
    "val = []\n",
    "for x, y in zip(val1, val2):\n",
    "    val.append([x, y])\n",
    "df = pd.DataFrame(val, columns = ['MSE', 'Proposed'])\n",
    "ax = sns.boxplot(data=df, palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "d['bestMSE'] = bestMSE\n",
    "d['bestCOMBO'] = bestCOMBO\n",
    "d['bestWRONG'] = bestWRONG \n",
    "d['worstMSE'] = worstMSE\n",
    "d['worstCOMBO'] = worstCOMBO\n",
    "d['worstWRONG'] = worstWRONG\n",
    "\n",
    "d['qbestMSE'] = qbestMSE\n",
    "d['qbestCOMBO'] = qbestCOMBO\n",
    "d['qbestWRONG'] = qbestWRONG \n",
    "d['qworstMSE'] = qworstMSE\n",
    "d['qworstCOMBO'] = qworstCOMBO\n",
    "d['qworstWRONG'] = qworstWRONG\n",
    "\n",
    "d['sbestMSE'] = sbestMSE\n",
    "d['sbestCOMBO'] = sbestCOMBO\n",
    "d['sbestWRONG'] = sbestWRONG \n",
    "d['sworstMSE'] = sworstMSE\n",
    "d['sworstCOMBO'] = sworstCOMBO\n",
    "d['sworstWRONG'] = sworstWRONG\n",
    "\n",
    "d['sqbestMSE'] = sqbestMSE\n",
    "d['sqbestCOMBO'] = sqbestCOMBO\n",
    "d['sqbestWRONG'] = sqbestWRONG \n",
    "d['sqworstMSE'] = sqworstMSE\n",
    "d['sqworstCOMBO'] = sqworstCOMBO\n",
    "d['sqworstWRONG'] = sqworstWRONG\n",
    "\n",
    "d['averageDegree']= averageDegree\n",
    "d['targetDegree'] = targetDegree\n",
    "d['target_inD']=  target_inD\n",
    "d['target_outD'] = target_outD\n",
    "d['descendants'] = descendants\n",
    "d['graphDiff']= graphDiff\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('18node.pkl', 'wb') as handle:\n",
    "    pickle.dump(d, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plots(X,Y):\n",
    "    box = []\n",
    "    for i in np.unique(X):\n",
    "        box.append([])\n",
    "    for x,y in zip(X, Y):\n",
    "        box[x].append(y)\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.set_xlabel(\"Number of Target Descendants\")\n",
    "    ax.set_ylabel(\"L1 Loss Improvement\")\n",
    "    bp = ax.boxplot(box, showfliers=False, labels = ['0', '1', '2'])\n",
    "    \n",
    "box_plots(descendants, improvement)\n",
    "box_plots(target_outD, improvement)\n",
    "box_plots(target_inD, improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_plot(x,y,z, xlab = 'Mean', ylab = 'Variance', zlab= 'Mean Generalization Error (MSE)', clim_low = 0, clim_high = 1):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cax = ax.scatter(x, y, c=z, s=70, edgecolor='black', cmap = 'hot')\n",
    "    cax.set_clim(clim_low, clim_high)\n",
    "    ax.set_xlabel(xlab)\n",
    "    ax.set_ylabel(ylab)\n",
    "    #plt.colorbar(cax)\n",
    "    cbar = plt.colorbar(cax)\n",
    "    cbar.set_label(zlab)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "heat_plot(target_outD, target_inD, normalize(improvement), xlab = 'inD', ylab='outD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(bestMSE), np.mean(bestCOMBO), np.std(bestMSE), np.std(bestCOMBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improvement\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(averageDegree,improvement, '.')\n",
    "plt.show()\n",
    "\n",
    "improvement\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(targetDegree,improvement, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(proposed,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(proposed,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(proposed,generalization, '.')\n",
    "plt.plot(proposed, b + m * np.array(proposed), '-')\n",
    "ax.set_xlabel(\"BIC\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(metrics,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(metrics,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(metrics,generalization, '.')\n",
    "plt.plot(metrics, b + m * np.array(metrics), '-')\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "total = normalize(metrics) + normalize(proposed)\n",
    "fig, ax = plt.subplots()\n",
    "b,m = polyfit(total,generalization, 1)\n",
    "text(0.05, 0.9,'Pearson coeff:' + str(pearsonr(total,generalization)[0])[0:4], ha='left', va='center', transform=ax.transAxes)\n",
    "ax.plot(total,generalization, '.')\n",
    "plt.plot(total, b + m * np.array(total), '-')\n",
    "ax.set_xlabel(\"MSE\")\n",
    "ax.set_ylabel(\"GEN\")\n",
    "fig.savefig('Ex4MSEVsAUS.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = 10\n",
    "final = pd.DataFrame(np.stack((metrics, proposed, total, generalization), axis = 1), columns = ['metrics', 'proposed', 'combined', 'generalization'])\n",
    "print(\"MSE = \", np.sum(final.nsmallest(nbest, 'metrics')['generalization']))\n",
    "print(\"BIC = \", np.sum(final.nsmallest(nbest, 'proposed')['generalization']))\n",
    "print(\"COMB = \",np.sum(final.nsmallest(nbest, 'combined')['generalization']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
